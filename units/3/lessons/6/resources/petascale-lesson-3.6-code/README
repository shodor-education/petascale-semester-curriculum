# Blue Waters Petascale Semester Curriculum v1.0
# Unit 3: Using a Cluster
# Lesson 6: Running Code on a Cluster 2
# File: README
# Developed by Mobeen Ludin for the Shodor Education Foundation, Inc.
#
# This file by The Shodor Education Foundation, Inc. is licensed under
# CC BY-SA 4.0. To view a copy of this license, visit
# <https://creativecommons.org/licenses/by-sa/4.0>.
#
# Browse and search the full curriculum at
# <http://shodor.org/petascale/materials/semester-curriculum>.
#
# We welcome your improvements! You can submit your proposed changes to this
# material and the rest of the curriculum in our GitHub repository at
# <https://github.com/shodor-education/petascale-semester-curriculum>.
#
# We want to hear from you! Please let us know your experiences using this
# material by sending email to petascale@shodor.org

This folder contains the examples userd to show how to compile and run different parallel code on Cedar
Supercompuuter of Compute Canada.

The examples folder contains:
    - mpi_example.c: this is a simple mpi program prints which MPI process is running on which Core of which node.

    - mpi_jobs.sbatch: SLURM batch script. This is an example of submitting jobs using batch scripts.
	To run the script:
		$ sbatch mpi_jobs.sbatch

    - omp_pi_area.c: Simple yet a more scientific example showing how to use OpenMP Reduction clause.
	This example is used to show how to compile and run OpenMP code on Cedar Compute Canada's cluster.
	$ make omp_pi_area

    - acc_laplace.c:  Simple yet a more scientific example showing how to use OpenACC compiler directives.
	This example will show how to swich compilers from intel to pgi, and compile and run OpenACC code
	$ module swap icc/.2018.3.222 pgi/19.4
	$ make acc_laplace

   - gpu_jobs.sbatch: this is an example of requesting nodes with gpus:
	$ sbatch gpu_jobs.sbatch
	The output should be in a file: slurm_$JOB_NAME

   - mpi_pi_area: Simple yet a more scientific example showing how to use collective communcation
	routines such as MPI_Bcast and MPI_Reduce as well as some MPI timing routines. This example will show
	how to compile and run code using openmpi/3.1.2 module.
	$ module load intel/2018.3 openmpi/3.1.2
	$ make mpi_pi_area

    - Makefile:   This make file could be used to compile and build the example codes.
        The make file can be modified to add any other compilers or platforms. It gives the option of selecting
        the platform and system and will compile with appropriate flags.


Pre-requisites: openmpi/3.1.2, intel/2018.3, and pgi/19.4

How to Load PGI Modules on Cedar Supercomputer:
    In order to compile CUDA or OpenACC cude on Cedar, you must use the PGI Compilers
    - To list currently loaded modules
		$ module list
OUTPUT:
Currently Loaded Modules:
  1) nixpkgs/16.09   (S)      3) gcccore/.5.4.0  (H)   5) ifort/.2016.4.258 (H)   7) openmpi/2.1.1 (m)
  2) imkl/11.3.4.258 (math)   4) icc/.2016.4.258 (H)   6) intel/2016.4      (t)   8) StdEnv/2016.4 (S)


	- To get list of all available modules
		$ module avail

	- To swap two modules
		$ module swap StdEnv/2016.4 StdEnv/2018.3
OUTPUT:
The following have been reloaded with a version change:
  1) StdEnv/2016.4 => StdEnv/2018.3         4) ifort/.2016.4.258 => ifort/.2018.3.222     7) openmpi/2.1.1 => openmpi/3.1.2
  2) gcccore/.5.4.0 => gcccore/.7.3.0       5) imkl/11.3.4.258 => imkl/2018.3.222
  3) icc/.2016.4.258 => icc/.2018.3.222     6) intel/2016.4 => intel/2018.3

NOTE: Each module might have dependencies. When you swap them, it will also swap the necessary dependency modules as well.
For example, we can see that 7) openmpi/2.1.1 was swapped in the new environemnt with the openmpi/3.1.2 release.

	- To get list of all pgi modules
		$ module spider pgi

	- To get more information on a specific version of the module
		$ module spider pgi/19.4

	- To load a specifi module version for a compiler
		$ module load pgi/19.4
OUTPUT:
Lmod is automatically replacing "intel/2018.3" with "pgi/19.4".

Inactive Modules:
  1) openmpi/3.1.2

NOTE: If you just load a module instead of swap, it will automatically unlaod the another version, or type of compiler/software
including its dependencies. In this case the modules program auto unloaded intel compiler, and since openmpi depends on intel
compiler, it became inactive.

